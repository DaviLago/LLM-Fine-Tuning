{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c9872c4",
   "metadata": {},
   "source": [
    "# LLM Fine-Tuning with Hugging Face, PEFT, and LoRA\n",
    "This notebook demonstrates how to fine-tune a language model using Hugging Face Transformers, PEFT, and LoRA on a large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eac0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b544697",
   "metadata": {},
   "source": [
    "### Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ded63e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U transformers datasets peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50445a4",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bceffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"DaviLago/AmazonTitles-1.3MM\", split=\"train\")\n",
    "dataset = dataset.select(range(5_000))\n",
    "print(f'Loaded {len(dataset)} records.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82b523c",
   "metadata": {},
   "source": [
    "### Hugging Face login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd922cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Login to Hugging Face\n",
    "login(new_session=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e8ac74",
   "metadata": {},
   "source": [
    "### Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ee63ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, DataCollatorForLanguageModeling\n",
    "\n",
    "# Define model name\n",
    "base_model = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "# Define quantization config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model, quantization_config=quantization_config, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=False)\n",
    "\n",
    "# Set padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Preprocess function\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c998e5b8",
   "metadata": {},
   "source": [
    "### Lora configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b8f71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Prepare PEFT/LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "# Integrate LoRA with the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Apply the accelerator. You can comment this out to remove the accelerator.\n",
    "model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b003afd",
   "metadata": {},
   "source": [
    "### Tokenized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd44aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess function\n",
    "def preprocess(example):\n",
    "    prompt = f\"Title \\\"{example['title']}\\\". Content, description and details about the title: \\\"{example['content']}\\\".\"\n",
    "    inputs = tokenizer(prompt, truncation=True, padding='max_length', max_length=256)\n",
    "    inputs['labels'] = inputs['input_ids'].copy()\n",
    "    return inputs\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = dataset.map(preprocess, batched=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e186cf8e",
   "metadata": {},
   "source": [
    "### Training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5a6773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Define output directory\n",
    "output_dir = \"/content/drive/MyDrive/amazon-titles-llama-finetuned\"\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=50,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    output_dir=output_dir,\n",
    "    save_total_limit=2,\n",
    "    save_steps=100,\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4010a61",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c9e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7dd451",
   "metadata": {},
   "source": [
    "### Save model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fcd30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a83d320",
   "metadata": {},
   "source": [
    "### Push model and tokenizer to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff6ad62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hugging Face model name\n",
    "hf_model_name = \"DaviLago/amazon-titles-llama-finetuned\"\n",
    "\n",
    "# Push to Hugging Face Hub\n",
    "model.push_to_hub(hf_model_name)\n",
    "tokenizer.push_to_hub(hf_model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
